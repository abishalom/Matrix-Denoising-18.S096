{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_ls(X, y):\n",
    "    inv = np.linalg.inv(np.dot(X.T, X))\n",
    "    a = np.dot(inv, X.T)\n",
    "    return np.dot(a, y)\n",
    "\n",
    "def solve_rr(X, y, lam):\n",
    "    inv = np.linalg.inv(np.dot(X.T, X) + lam * np.identity(np.shape(X)[1]))\n",
    "    a = np.dot(inv, X.T)\n",
    "    return np.dot(a, y)\n",
    "\n",
    "#John's ALS code\n",
    "def ALS(data_train, data_test, k=2, lam=0.02, max_iter=100):\n",
    "    '''\n",
    "    data_train and data_test are lists of tuples (row, column, MSE)\n",
    "    these need to be integers so we need to map each dataset and lambda value to an integer (MSE need not be integer)\n",
    "    \n",
    "    k is rank\n",
    "    lam is the regularizer on the least squares call in each step\n",
    "    \n",
    "    my code also has offsets b_u and b_v\n",
    "    the prediction for entry (a, i) is (b_u[a] + b_v[i] + np.dot(u[a].T, v[i])[0][0])\n",
    "    '''\n",
    "    # size of the problem\n",
    "    n = max(d[0] for d in data_train)+1 # datasets\n",
    "    m = max(d[1] for d in data_train)+1 # lambda values\n",
    "    # which entries are set in each row and column and the MSE\n",
    "    us_from_v = [[] for i in range(m)]  # II (i-index-set)\n",
    "    vs_from_u = [[] for a in range(n)]  # AI (a-index set)\n",
    "    for (a, i, r) in data_train:\n",
    "        us_from_v[i].append((a, r))\n",
    "        vs_from_u[a].append((i, r))\n",
    "    # Initial guesses for u, b_u, v, b_v\n",
    "    # Note that u and v are lists of column vectors (rows of U, V).\n",
    "    u, b_u, v, b_v = ([np.random.normal(1/k, size=(k,1)) for a in range(n)],\n",
    "          np.zeros(n),\n",
    "          [np.random.normal(1/k, size=(k,1)) for i in range(m)],\n",
    "          np.zeros(m))\n",
    "    for itr in range(max_iter):\n",
    "        if itr%5 == 0:\n",
    "            print(itr)\n",
    "        for i in range(len(u)): #run ls on u\n",
    "            X_mat = np.array([np.append(np.array([1]), v[a[0]].T[0]) for a in vs_from_u[i]])\n",
    "            y_vec = np.array([[a[1] - b_v[a[0]]] for a in vs_from_u[i]])\n",
    "            if len(X_mat) > 0:\n",
    "                sol = solve_rr(X_mat, y_vec, lam)\n",
    "                b_u[i] = sol[0]\n",
    "                u[i] = sol[1:]\n",
    "        for j in range(len(v)): #run ls on v\n",
    "            X_mat = np.array([np.append(np.array([1]), u[a[0]].T[0]) for a in us_from_v[j]])\n",
    "            y_vec = np.array([[a[1] - b_u[a[0]]] for a in us_from_v[j]])\n",
    "            if len(X_mat) > 0:\n",
    "                sol = solve_rr(X_mat, y_vec, lam)\n",
    "                b_v[j] = sol[0][0]\n",
    "                v[j] = sol[:][1:]\n",
    "            \n",
    "    # TODO: Evaluate using some error metric measured on test set\n",
    "    error = 0\n",
    "    count = 0\n",
    "    for (a, i, r) in data_test:\n",
    "        count += 1\n",
    "        error += (b_u[a] + b_v[i] + np.dot(u[a].T, v[i])[0][0] - r) ** 2\n",
    "    error = error / count\n",
    "    return (u, b_u, v, b_v, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    dataset_dic = {}\n",
    "    for i in range(1, 101):\n",
    "        X_df = pd.read_csv('gendata/example_dataset' + str(i) + '.csv')\n",
    "        dataset_dic[i - 1] = X_df\n",
    "    return dataset_dic\n",
    "\n",
    "lambda_dic = {lamb + 60: 10 ** (-.05 * lamb) for lamb in range(-60, 40)}\n",
    "\n",
    "def get_MSE(X_df, lam, iters = 1, size = 0.5):\n",
    "    MSE = 0\n",
    "    for _ in range(iters):\n",
    "        train, test = train_test_split(X_df, test_size=size)\n",
    "        X_train, Y_train = np.array(train.drop('Y', axis=1)), np.transpose(np.array([train['Y']]))\n",
    "        X_1 = np.hstack((np.ones(shape = (len(X_train), 1)), X_train))\n",
    "        sol = solve_rr(X_1, Y_train, lam)\n",
    "        X_test, Y_test = np.array(test.drop('Y', axis=1)), np.transpose(np.array([test['Y']]))\n",
    "        X_2 = np.hstack((np.ones(shape = (len(X_test), 1)), X_test))\n",
    "        MSE += np.linalg.norm(X_2@sol - Y_test) / len(X_2)\n",
    "    return MSE / iters\n",
    "  \n",
    "def get_points(m, n, p, q):\n",
    "    train_points = set()\n",
    "\n",
    "    for i in range(m):\n",
    "        j = np.random.randint(0, n)\n",
    "        train_points.add((i, j))\n",
    "    for j in range(n):\n",
    "        i = np.random.randint(0, m)\n",
    "        if (i, j) not in train_points:\n",
    "            train_points.add((i, j))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if np.random.random() < p:\n",
    "                if (i, j) not in train_points:\n",
    "                    train_points.add((i, j))\n",
    "\n",
    "    test_points = set()\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if np.random.random() < q:\n",
    "                if (i, j) not in train_points:\n",
    "                    test_points.add((i, j))\n",
    "                    \n",
    "    return train_points, test_points\n",
    "\n",
    "def generate_data(dataset_dic, lambda_dic, points):\n",
    "    data = []\n",
    "    for (i, j) in points:\n",
    "        MSE = get_MSE(dataset_dic[i], lambda_dic[j])\n",
    "        data.append((i, j, MSE))                    \n",
    "    return data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded datasets\n",
      "generated points\n",
      "generated train list\n",
      "generated test list\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "finished ALS\n"
     ]
    }
   ],
   "source": [
    "dataset_dic = load_datasets()\n",
    "print('loaded datasets')\n",
    "train_pts, test_pts = get_points(len(dataset_dic), len(lambda_dic), p = 0.01, q = 0.02)\n",
    "print('generated points')\n",
    "data_train = generate_data(dataset_dic, lambda_dic, train_pts)\n",
    "print('generated train list')\n",
    "data_test = generate_data(dataset_dic, lambda_dic, test_pts)\n",
    "print('generated test list')\n",
    "u, b_u, v, b_v, error = ALS(data_train, data_test, k=2, lam=0.02, max_iter=100)\n",
    "print('finished ALS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027754076322686425"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 95, 0.011600826280359878),\n",
       " (73, 32, 0.09305301756965943),\n",
       " (4, 41, 0.12634204687968942),\n",
       " (20, 25, 0.1081172817168087),\n",
       " (26, 97, 0.0010073850896873194),\n",
       " (72, 11, 0.30466693460579586),\n",
       " (97, 16, 0.20168114146994046),\n",
       " (37, 82, 0.15901251173673908),\n",
       " (52, 94, 0.03197963617572955),\n",
       " (32, 2, 0.24130219820650517),\n",
       " (4, 93, 0.16258596204547757),\n",
       " (70, 19, 0.08717893758683658),\n",
       " (10, 69, 0.02144003772131511),\n",
       " (80, 81, 0.1322487919679641),\n",
       " (89, 94, 0.11557139319720808),\n",
       " (90, 27, 0.1172102119693704),\n",
       " (73, 35, 0.09257703179540666),\n",
       " (96, 71, 0.06503473224253728),\n",
       " (41, 29, 0.1557443491043857),\n",
       " (57, 45, 0.08702084279065471),\n",
       " (74, 56, 0.1382932913665159),\n",
       " (19, 76, 0.11522379063824649),\n",
       " (51, 22, 0.14448418865103535),\n",
       " (91, 8, 0.1265299239386899),\n",
       " (6, 44, 0.09829802153491456),\n",
       " (13, 73, 0.009106476908805165),\n",
       " (22, 65, 0.17694525896471783),\n",
       " (67, 46, 0.03948789872403857),\n",
       " (74, 73, 0.12910652112214044),\n",
       " (18, 86, 0.07617381335486088),\n",
       " (85, 47, 0.12628920730605248),\n",
       " (72, 44, 0.10994521292323535),\n",
       " (12, 45, 0.14453062379521467),\n",
       " (84, 95, 0.048337688744071426),\n",
       " (63, 58, 0.11419463066394375),\n",
       " (60, 79, 0.06688887622573637),\n",
       " (41, 11, 0.1398428133097294),\n",
       " (82, 19, 0.23522212486402475),\n",
       " (5, 21, 0.1469969771162908),\n",
       " (46, 32, 0.08401966132710229),\n",
       " (82, 92, 0.01958079417555025),\n",
       " (89, 1, 0.3594705527892558),\n",
       " (91, 17, 0.12339559246430479),\n",
       " (21, 84, 0.10391778628628216),\n",
       " (65, 56, 0.12233269188330338),\n",
       " (25, 63, 0.16940491437256167),\n",
       " (72, 86, 0.15169453560832077),\n",
       " (20, 63, 0.0933241233688782),\n",
       " (53, 92, 0.15521962600097208),\n",
       " (90, 32, 0.10780745342881423),\n",
       " (34, 20, 0.11097604970960201),\n",
       " (70, 84, 0.04247827568777006),\n",
       " (10, 91, 0.009494119705093059),\n",
       " (26, 43, 0.022345696917932714),\n",
       " (68, 52, 0.13079506622251402),\n",
       " (5, 93, 0.1305446262276722),\n",
       " (82, 97, 0.01858181074172321),\n",
       " (46, 95, 0.04520186962095556),\n",
       " (5, 37, 0.1119346766705511),\n",
       " (56, 82, 0.11322474280487925),\n",
       " (72, 77, 0.10312630262395618),\n",
       " (97, 86, 0.05949969426719483),\n",
       " (98, 19, 0.19734507700762907),\n",
       " (41, 40, 0.14795926530172362),\n",
       " (6, 42, 0.13610661698469637),\n",
       " (39, 30, 0.061929834194431994),\n",
       " (53, 95, 0.200599209487689),\n",
       " (77, 61, 0.12368572016909427),\n",
       " (41, 50, 0.14881697358295085),\n",
       " (83, 64, 0.0590481728391482),\n",
       " (4, 84, 0.15087669572927484),\n",
       " (50, 97, 0.2160661619780026),\n",
       " (97, 34, 0.07999739761408312),\n",
       " (20, 26, 0.11159692760218214),\n",
       " (12, 79, 0.16328117165147404),\n",
       " (84, 76, 0.04680524269717687),\n",
       " (31, 18, 0.2250499881664743),\n",
       " (36, 18, 0.1715202877119599),\n",
       " (91, 12, 0.145395477592654),\n",
       " (46, 13, 0.21395599590462056),\n",
       " (86, 78, 0.15844964307279008),\n",
       " (51, 15, 0.1302818141322179),\n",
       " (15, 61, 0.009926413559017463),\n",
       " (40, 61, 0.06849930694387926),\n",
       " (63, 23, 0.11085964528667401),\n",
       " (29, 26, 0.11681847923905106),\n",
       " (28, 6, 0.22370281018145602),\n",
       " (78, 87, 0.18726599310288866),\n",
       " (88, 8, 0.2228151571148698),\n",
       " (95, 93, 0.17595922932490676),\n",
       " (76, 78, 0.08324165489508498),\n",
       " (31, 4, 0.30199151653283873),\n",
       " (81, 74, 0.15141609615860677),\n",
       " (61, 31, 0.18684610125377324),\n",
       " (60, 9, 0.23635743113200575),\n",
       " (66, 84, 0.1220376033892339),\n",
       " (94, 74, 0.15588934828104167),\n",
       " (23, 96, 0.09160978786622605),\n",
       " (65, 60, 0.11346056211191088),\n",
       " (58, 83, 0.028702051666958356),\n",
       " (44, 98, 0.0011230984711001874),\n",
       " (59, 23, 0.1101941491933128),\n",
       " (65, 86, 0.10418104219583617),\n",
       " (97, 54, 0.058740300218976964),\n",
       " (29, 72, 0.09759654750663692),\n",
       " (76, 64, 0.08610618343441574),\n",
       " (95, 18, 0.31379713334791765),\n",
       " (79, 61, 0.11477190818748034),\n",
       " (2, 91, 0.06713596442552747),\n",
       " (0, 2, 0.30416190526808945),\n",
       " (48, 81, 0.1345549517817463),\n",
       " (71, 67, 0.024347432531044863),\n",
       " (38, 93, 0.10885876610147513),\n",
       " (45, 14, 0.1848074096863523),\n",
       " (60, 42, 0.08034154946220547),\n",
       " (13, 37, 0.07730437301258225),\n",
       " (1, 37, 0.13456574942636984),\n",
       " (24, 99, 0.036436848328064306),\n",
       " (77, 81, 0.1177759523117408),\n",
       " (5, 26, 0.15458392052862277),\n",
       " (79, 7, 0.37395610387928435),\n",
       " (3, 38, 0.07105188692380517),\n",
       " (81, 22, 0.1761276142581201),\n",
       " (77, 94, 0.1134978530861474),\n",
       " (48, 51, 0.12848534397724967),\n",
       " (97, 96, 0.06043288569190815),\n",
       " (69, 72, 0.1019350308288399),\n",
       " (25, 23, 0.1699446800759147),\n",
       " (39, 3, 0.11523623647914612),\n",
       " (72, 22, 0.17628244339848165),\n",
       " (50, 4, 0.4103222566212288),\n",
       " (32, 10, 0.22048854991942687),\n",
       " (37, 59, 0.18170939044599613),\n",
       " (29, 78, 0.07762812628791056),\n",
       " (26, 45, 0.023024493524294358),\n",
       " (47, 48, 0.09551111424939433),\n",
       " (81, 78, 0.15515004006180494),\n",
       " (97, 33, 0.07149627653157499),\n",
       " (76, 85, 0.0860019756111622),\n",
       " (62, 85, 0.11515516867266393),\n",
       " (0, 78, 0.004076925267360177),\n",
       " (58, 96, 0.03200245886256442),\n",
       " (38, 10, 0.16341431815395108),\n",
       " (98, 77, 0.14723630453198833),\n",
       " (61, 55, 0.03536886404320131),\n",
       " (46, 67, 0.04906139997503832),\n",
       " (96, 62, 0.06558414369064641),\n",
       " (77, 63, 0.12404764555145077),\n",
       " (53, 57, 0.16561212328930688),\n",
       " (85, 20, 0.15018138161299988),\n",
       " (15, 21, 0.07502014126116237),\n",
       " (30, 87, 0.0459746373825286),\n",
       " (86, 69, 0.14107386780175),\n",
       " (15, 75, 0.004061478121130163),\n",
       " (14, 58, 0.11427578725415824),\n",
       " (74, 0, 0.4439719391056224),\n",
       " (46, 73, 0.053604488943145824),\n",
       " (93, 69, 0.12536523853973944),\n",
       " (26, 57, 0.015106639787623177),\n",
       " (10, 73, 0.012350006005710353),\n",
       " (58, 18, 0.08152164765694134),\n",
       " (65, 85, 0.12138870430768604),\n",
       " (29, 28, 0.10970875712751044),\n",
       " (28, 4, 0.255235323743418),\n",
       " (23, 74, 0.09100728465460887),\n",
       " (78, 68, 0.1608773763929836),\n",
       " (35, 84, 0.06056220030017932),\n",
       " (80, 33, 0.11898581580385269),\n",
       " (11, 23, 0.025828110638396688),\n",
       " (96, 17, 0.14541908126452446),\n",
       " (75, 33, 0.13887812463745378),\n",
       " (27, 95, 0.12786437756474606),\n",
       " (92, 76, 0.09480955336066327),\n",
       " (14, 55, 0.09341343271358529),\n",
       " (97, 94, 0.05811912849746034),\n",
       " (65, 35, 0.10246719621980059),\n",
       " (34, 18, 0.11436567096885497),\n",
       " (44, 96, 0.0015310488199921696),\n",
       " (65, 80, 0.09848204141860442),\n",
       " (53, 87, 0.2040765118875388),\n",
       " (70, 74, 0.03683704233061268),\n",
       " (82, 69, 0.023905421423502167),\n",
       " (15, 97, 0.003461385397645147),\n",
       " (14, 27, 0.10334503122994586),\n",
       " (42, 50, 0.15091055560848052),\n",
       " (99, 38, 0.1410557938315396),\n",
       " (6, 22, 0.12898485019339562),\n",
       " (46, 35, 0.08015832552998242),\n",
       " (61, 89, 0.05778541547634453),\n",
       " (60, 12, 0.24618629579705695),\n",
       " (17, 14, 0.06884164426269294),\n",
       " (37, 30, 0.1808944230291361),\n",
       " (44, 2, 0.26890937042249713),\n",
       " (52, 87, 0.036427787981056725),\n",
       " (32, 96, 0.01210989071589911),\n",
       " (58, 91, 0.03513890722729792),\n",
       " (84, 55, 0.04548164506750283),\n",
       " (85, 97, 0.13278747991229015),\n",
       " (7, 45, 0.13731446622808757),\n",
       " (25, 45, 0.1642325875663119),\n",
       " (25, 96, 0.17310990968437084),\n",
       " (57, 77, 0.0796510981419224),\n",
       " (80, 21, 0.15274651537195552),\n",
       " (91, 42, 0.08893949637891661),\n",
       " (10, 90, 0.010629789890157813),\n",
       " (23, 76, 0.09084672329946517),\n",
       " (20, 23, 0.14640021765995465),\n",
       " (79, 54, 0.1267343420430408),\n",
       " (93, 18, 0.142776225958479),\n",
       " (15, 87, 0.0019015245600000864),\n",
       " (2, 21, 0.22246683475794424),\n",
       " (23, 23, 0.2114922786683181),\n",
       " (51, 79, 0.12082973212793367),\n",
       " (18, 29, 0.10076995332557029),\n",
       " (33, 70, 0.1823310784470501),\n",
       " (21, 69, 0.10464827795136232),\n",
       " (31, 55, 0.08141698784863427),\n",
       " (70, 77, 0.02815477501955043),\n",
       " (77, 62, 0.1170061479056947),\n",
       " (93, 57, 0.11888030447681669),\n",
       " (21, 50, 0.09504031102870725),\n",
       " (71, 98, 0.025589934366697945)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
